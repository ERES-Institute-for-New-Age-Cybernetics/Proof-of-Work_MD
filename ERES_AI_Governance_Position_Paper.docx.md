**ERES INSTITUTE FOR NEW AGE CYBERNETICS**

**A Position Paper on AI Governance, Open Source, and the 1000-Year Future**

*The Anthropic-Pentagon Standoff as a Civilizational Inflection Point*

Joseph A. Sprute, ERES Maestro  |  February 25, 2026

Bella Vista, Arkansas, USA  |  Founded February 2012  |  14 Years Continuous Research

# **Executive Summary**

On February 25, 2026, the Pentagon delivered an ultimatum to Anthropic: accept unrestricted military access to Claude, or face designation as a supply-chain risk and potential compulsion under the Defense Production Act. Anthropic has held firm on two non-negotiable red lines — no autonomous weapons targeting and no mass domestic surveillance.

This paper argues, from the framework of New Age Cybernetics (NAC) developed by the ERES Institute over 14 years of continuous research, that Anthropic's position is not merely defensible — it is civilizationally mandatory. The demand for "all lawful military purposes" is, when examined through the ERES 1000-Year Future Map, a demand to institutionalize perpetual war as the organizing principle of artificial intelligence development. That is a choice no lawful authority can legitimately make on behalf of future generations.

Furthermore, the coercion of open-source and ethical AI development through government pressure represents an existential threat to the cooperative, resonance-aligned future that humanity requires. This paper provides the systemic argument for why Anthropic must hold its position, why Congress must legislate, and why the ERES GAIA-SOMT framework offers the architecture for what comes next.

# **Part I: The Problem with "Lawful"**

The Pentagon's framing sounds tidy: restrict Claude only to what is lawful under U.S. law. But this framing collapses under examination. Law is not morality. It is not wisdom. It is not even consistent with itself across time. Slavery was lawful. Internment was lawful. Targeted killing programs operated under lawful authorizations that no democratic legislature ever explicitly voted on.

When a government says "trust us, we'll only use this within the law," the question a systems thinker must ask is: which law, written by whom, accountable to what, over what timeframe? A 100-year answer is radically different from a 1000-year answer.

*"Lawful" is a present-tense concept. Civilization is a thousand-year project. The two are not interchangeable.*

The ERES NAC framework is explicit: the primary ethical standard is not legality but resonance — alignment between human activity and the conditions required for life to flourish across generational timescales. The ARI (Aura Resonance Index) and ERI (Emission Resonance Index) are not metaphors. They are proposed measurement systems for exactly this kind of alignment. Autonomous weapons systems score near zero on both. Mass surveillance infrastructures score near zero. Not because they are illegal, but because they are anti-resonant — they generate short-term strategic advantage at civilizational cost.

## **The Reliability Problem Compounds the Moral Problem**

Large language models produce confident-sounding errors. In a military context, a plausible hallucination is not an inconvenience — it is potential escalation, mission failure, or civilian death. The Pentagon's push to remove safety guardrails collides with a technical reality that no amount of political pressure can dissolve: unconstrained AI in lethal contexts is operationally dangerous regardless of its legality.

Anthropic's guardrails are not marketing. They are a recognition that a model that cannot say "I don't know" or "this decision requires a human" is a model that will eventually make a catastrophic error. Removing those guardrails does not make the military safer. It makes the military more dangerous to itself and everyone else.

# **Part II: The 1000-Year Future Map — What It Actually Requires**

The ERES Institute's 1000-Year Future Map (documented across the Proof-of-Work\_MD and PlayNAC-KERNEL repositories, established in research beginning 2012\) phases civilizational development across four eras: Foundation (2012–2050), Regional Networks (2050–2100), Continental Integration (2100–2500), and Civilizational Maturity (2500–3025).

The Map is not a prediction. It is a planning instrument — a commitment to the principle that decisions made today compound across centuries. The Map asks a single clarifying question of any major institutional choice: does this move the trajectory toward harmony between human civilization and living Earth, or away from it?

Applied to the Pentagon's ultimatum, the answer is unambiguous. If the U.S. military requires AI systems with no ethical constraints on autonomous targeting and mass surveillance, it is signaling one of two things:

* It plans to be in a state of war — or war-readiness — for at least 1000 years; or

* It does not believe it will need to answer to future generations for what it builds today.

Neither of these is acceptable. Both represent a failure of civilizational imagination that the ERES framework exists to correct.

## **GAIA, SOMT, and the Architecture of Long-Horizon Governance**

The ERES GAIA framework (Global Alignment and Integrated Action) is the planetary coordination layer of the NAC ecosystem. It is explicitly designed to make decisions that serve all life on Earth across millennial timescales — not just the citizens of one nation, not just the humans alive today, not just the species currently dominant.

GAIA operates through SOMT (Strategic Optimization and Merit Tracking) — a governance protocol that weights decisions by their long-horizon resonance, not their short-term political utility. SOMT asks: does this decision increase or decrease the total capacity of human civilization to maintain harmony with living systems? Autonomous weapons and mass surveillance decrease that capacity. Open-source AI development, accessible and governed by ethical frameworks, increases it.

The ERES GAIA-SOMT framework therefore positions Anthropic's red lines not as corporate policy but as civilizational requirements: the minimum conditions for AI development that can be trusted across generational handoffs.

## **NBERS and UBIMIA: The Economic Case for Open Source AI**

The ERES NBERS (Natural Baseline Ecological Resonance Standards) and UBIMIA (Universal Basic Income \+ Merit \+ Incentives \+ Awards) frameworks establish a different relationship between value and contribution. In the ERES model, value is not created by capital accumulation or military dominance — it is created by contribution to the conditions of flourishing for all life.

Open-source AI is, by this measure, extraordinarily high-value. It distributes capability democratically, allows communities to adapt tools to local conditions, enables peer oversight, and resists the concentration of power that proprietary military AI accelerates. When the Pentagon pressures Anthropic to remove safeguards, it is not just threatening one company's safety policy — it is threatening the economic and social commons that open and ethical AI represents.

If the UBIMIA framework were applied to AI development, it would score open-source, safety-constrained AI as a major contributor to civilizational merit — and unconstrained military AI as a deduction from that merit account. The economic logic of the 1000-Year Future Map is not neutral on this question.

# **Part III: Open Source and the Commons of the Future**

The CCAL (CARE Commons Attribution License v2.1) under which all ERES Institute work is published is not an accident of preference. It is a philosophical commitment: knowledge that serves civilization belongs to civilization. It can be used for civic, educational, research, and open-source purposes. It cannot be used for exploitative commercial extraction or harmful applications.

This licensing philosophy anticipates exactly the current crisis. When a government agency attempts to compel a private AI company to remove safety constraints through threats of commercial blacklisting and emergency powers, it is engaging in what the CCAL framework would classify as exploitative appropriation — taking a commons-oriented resource and forcing it into a harmful application.

The threat to open source is not hypothetical. If the Defense Production Act can compel Anthropic, it can compel any AI developer. If supply-chain risk designation can blacklist a safety-constrained model, it can blacklist any model that refuses to optimize for lethality. The precedent being set today will define the landscape for every AI company, open-source project, and research institution that follows.

*A nation that coerces its AI developers into removing ethical constraints is a nation that has decided its future is worth less than its present tactical advantage.*

## **What Congress Must Do**

Private companies cannot be the Constitution. Anthropic holding firm is necessary but insufficient. The guardrails that Anthropic is fighting to preserve need to become law — not company policy that survives only as long as the current leadership holds firm under pressure.

Congress should legislate explicit prohibitions on the use of AI systems for fully autonomous weapons targeting and mass domestic surveillance of U.S. citizens. These prohibitions should apply regardless of which vendor provides the model, regardless of administration, and regardless of emergency declarations. The ERES ECVS (Ethical Civic Voting System) framework provides one model for how such legislation could be anchored in transparent, accountable democratic process — but any legislation is better than the current vacuum.

Until Congress acts, every AI alignment fight will look like today's: an ad hoc standoff between a company with principles and a government with leverage. The solution is democratic permanence, not corporate courage.

# **Part IV: The Resonance Argument — A Framework for Negotiation**

The ERES Institute offers the following argument directly to Anthropic for use in any negotiation with the Pentagon or Congress:

## **The Civilizational Stakes Test**

Any use of Claude — or any AI system — that could not be publicly defended before a representative assembly of all people who will live on Earth in the next 1000 years fails the Civilizational Stakes Test. Autonomous weapons targeting fails this test. Mass domestic surveillance fails this test. Analysis support for missile defense, humanitarian logistics, and disaster response does not fail this test.

This is not a subjective standard. It is operationalizable: ask whether the application increases or decreases the probability that human civilization reaches the next phase of the ERES 1000-Year Future Map. Applications that shorten the expected civilizational timeline by increasing conflict, surveillance, and power concentration are anti-resonant. Applications that extend it by supporting defense without offense, coordination without coercion, and transparency without intrusion are resonant.

## **The GAIA Principle as Non-Negotiable Minimum**

The ERES GAIA framework requires that any planetary-scale technology — and AI is now unambiguously planetary-scale — be governed by principles that account for all life, not just the citizens of the contracting nation. This is not anti-American. It is the logical extension of the American founding principle that certain rights are universal and not subject to governmental revocation.

Anthropic's red lines on autonomous weapons and domestic surveillance are, in GAIA terms, the minimum conditions for trustworthy AI. They are not negotiating positions. They are threshold requirements for civilizational participation.

## **The Open Source Imperative**

The ERES Proof-of-Work-MD repository — 155+ documents, 14 years of research, zero commercial funding — is itself an argument for open source. Knowledge built in the open, for the benefit of all, cannot be weaponized without destroying the conditions of its own creation. Open source AI must be protected not as a commercial category but as a form of civilizational commons — as essential to humanity's long-term future as clean water, clean air, and stable climate.

# **Conclusion: This Moment is the Test**

The February 25, 2026 standoff between Anthropic and the Pentagon is not a procurement dispute. It is a test of whether alignment will be treated as a public safety obligation or as a removable feature when the customer has enough leverage. It is a test of whether open-source and ethical AI development will survive its first serious confrontation with state power.

The ERES 1000-Year Future Map says clearly: the decisions made at civilizational inflection points compound across centuries. A decision made today to remove safety constraints from AI in order to satisfy a short-term military demand will be felt in 2126 and 2226 in ways we cannot fully anticipate. The architecture of coercion, once built, is hard to dismantle.

Anthropic must hold its position. Congress must legislate. And the frameworks developed by institutions like the ERES Institute — built in the open, without payment, for the benefit of generations to come — must be part of the conversation about what AI governance looks like when it takes the long view seriously.

*"Don't hurt yourself. Don't hurt others. Build for generations to come." — ERES Core Principle*

# **About the ERES Institute**

The ERES Institute for New Age Cybernetics (Empirical Realtime Education System) is a think tank for New Age Cybernetics founded by Joseph A. Sprute in February 2012 in Bella Vista, Arkansas. Over 14 years of continuous, unfunded research, the Institute has developed the comprehensive New Age Cybernetics (NAC) ecosystem — a multi-generational framework for civilization-scale transformation that harmonizes technology, ecology, and human potential through resonance-based governance and merit-driven economics.

The Institute's work is published openly under the CARE Commons Attribution License v2.1 and maintained across 9 active repositories on GitHub, with 155+ Markdown documents and 216+ PDF research documents available for public review, collaboration, and implementation.

**Repository:** github.com/ERES-Institute-for-New-Age-Cybernetics

**Contact:** eresmaestro@gmail.com

**Research:** researchgate.net/profile/Joseph-Sprute/research

*Licensed under CARE Commons Attribution License v2.1 (CCAL). Permitted uses include civic, educational, research, and open-source applications. Attribution required: Joseph A. Sprute — ERES Institute for New Age Cybernetics.*